# ğŸ–ï¸ Vacation Rental Address Scraper - Replit Quick Start

Welcome! This tool scrapes vacation rental websites to extract property addresses.

## ğŸš€ Quick Start (2 minutes)

### First Time Setup

1. **Install Playwright browsers** (one-time only):
   ```bash
   playwright install chromium
   ```

2. **Test the scraper** with a quick URL:
   ```bash
   python scripts/run_scrape.py \
       --url https://www.30aescapes.com/search/for-rent \
       --manager-name "30A Escapes" \
       --manager-domain 30aescapes.com \
       --market-name "30A / Destin, FL"
   ```

3. **Export your results**:
   ```bash
   python scripts/export_csv.py --run-id 1 --output output/my_results.csv
   ```

4. **View the CSV** - Check the `output/` folder!

## ğŸ“‹ Two Ways to Use This Tool

### Option A: Quick "Paste URL" Mode âš¡
Perfect for one-off scrapes - no config file needed!

```bash
python scripts/run_scrape.py \
    --url https://example.com/vacation-rentals \
    --manager-name "Manager Name" \
    --manager-domain example.com \
    --market-name "City, State"
```

### Option B: Config File Mode ğŸ“
Better for repeated scraping - create a YAML config:

```bash
# Edit configs/my_site.yaml first, then:
python scripts/run_scrape.py --config configs/my_site.yaml
```

See `configs/example_site.yaml` for a template.

## ğŸ“Š What You'll Get

The scraper will:
- âœ… Discover all listing pages (handles pagination, infinite scroll)
- âœ… Extract addresses using multiple strategies (JSON-LD, selectors, patterns)
- âœ… Normalize addresses into structured fields (street, city, state, zip)
- âœ… Export clean CSV with one row per property

## ğŸ—„ï¸ Database

All data is stored in `data/scraper.db` (SQLite). You can:
- Run multiple scrapes (each gets a unique run ID)
- Export any previous scrape using its run ID
- Keep full history of all scraping sessions

## ğŸ› ï¸ Useful Commands

```bash
# See all options
python scripts/run_scrape.py --help

# Debug mode (verbose logging)
python scripts/run_scrape.py --config configs/my_site.yaml --log-level DEBUG

# Export with custom filename
python scripts/export_csv.py --run-id 2 --output output/beach_properties.csv
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_scrape.py    # Main scraping script
â”‚   â””â”€â”€ export_csv.py    # CSV export utility
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ *.yaml           # Site configurations
â”œâ”€â”€ data/
â”‚   â””â”€â”€ scraper.db       # SQLite database
â”œâ”€â”€ output/
â”‚   â””â”€â”€ *.csv            # Exported results
â””â”€â”€ src/                 # Core scraping logic
```

## ğŸ’¡ Tips

- **Start small**: Test with 1-2 pages before scraping entire sites
- **Be polite**: Use delays (configured in YAML) to avoid overwhelming servers
- **Check robots.txt**: Make sure you're allowed to scrape the target site
- **Network issues**: If scraping fails, it might be network/proxy issues in Replit

## ğŸ”§ Troubleshooting

**"Module not found" error?**
```bash
pip install -r requirements.txt
playwright install chromium
```

**"net::ERR_TUNNEL_CONNECTION_FAILED"?**
- This is a Replit network restriction (can't access external HTTPS sites from headless browser)
- Run the scraper locally or on a server instead

**No addresses extracted?**
- Check if the site has addresses visible on listing pages
- Try adding specific CSS selectors in your config file
- Use `--log-level DEBUG` to see what's being extracted

## ğŸ“š Full Documentation

See the main [README.md](./README.md) for complete documentation.

---

**Ready to scrape?** Start with the Quick Start commands above! ğŸš€
